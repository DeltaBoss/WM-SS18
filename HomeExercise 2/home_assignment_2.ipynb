{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups of 2 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to L2P by 26th of June 23:59h. (The deadline is strict)\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group\n",
    "Deniz Schmidt 334744\n",
    "Dinc Erduran 262999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dynamic PageRank\n",
    "Consider a random walk setting where the transistion matrix changes over time. At any point of time the probability of a random surfer to jump to a linked page is proportional to the number of previous visits. To start with all the pages are equally likely to be chosen but as the walk continues and the nodes are visited the transition probability changes as proportional to number of previous visits. For example let a page 'a' is linked to pages 'b', 'c' and 'd'. The random surfer currently resides at 'a' and the pages 'b', 'c' and 'd' have already been visited 5, 3 and 2 times respectively. The transition probability would be 0.5, 0.3 and 0.2 respectively. As a new node is viited the probabilities change. The random surfer continues to surf with probability 0.8. Generate 100 random walks and rank the nodes based on the frequency of visit. The random walk should be performed on a drected Erdos-Renyi graph with number of nodes n=200 and probability of edge creation p = 0.4. \n",
    "\n",
    "Hint: Use networkx library for generating graph.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import nx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "## generate (directed) GNP random graph\n",
    "G = nx.MultiDiGraph(nx.gnp_random_graph(200, 0.4, directed=True))\n",
    "visit_freq_map = defaultdict(int) # to count node visits\n",
    "\n",
    "for i in range(100):\n",
    "    H = G.copy()\n",
    "    currNode = 0\n",
    "    while(random.random() >= 0.2):\n",
    "        new_link = list(H.edges(currNode))[random.randint(0, len(H.edges(currNode))-1)]\n",
    "        H.add_edge(new_link[0], new_link[1])  # to increase visit probability\n",
    "        currNode = new_link[1]\n",
    "        visit_freq_map[currNode] += 1\n",
    "\n",
    "# print out nodes by frequency of visit in descending order\n",
    "print(\"nodes by frequency of visit:\")\n",
    "print(sorted(visit_freq_map.items(), key=lambda kv: kv[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendation\n",
    "a. Compare the recommendation algorithms (SVD, NMF, Baseline, k-NN and Random) available in surprise package on movielens dataset in terms of RMSE and MAE.\n",
    "\n",
    "b. Consider the movielens dataset and divide it into (i) training set with 50% of the data (train the algorithms on this part) and (ii) 25% validation set and (iii) test set with the rest. Estimate the ratings of the test set using the algorithms (same as in a) provided by the package on the training set. Your final rating should be weighted average of the ratings predicted by the algorithms. The weights should be learnt on the validation set. Performance should be measured in terms of RMSE.\n",
    "\n",
    "Hint: Use grid search/step-wise update like SGD for learning the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import SVD\n",
    "from surprise import NMF\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# 5-fold cross validation results for 5 algorithms \n",
    "cross_validate(SVD(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "cross_validate(NMF(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "cross_validate(surprise.prediction_algorithms.baseline_only.BaselineOnly(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "cross_validate(surprise.prediction_algorithms.knns.KNNBasic(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "cross_validate(surprise.prediction_algorithms.random_pred.NormalPredictor(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# Comparison on RMSE, MAE with 5-fold cv result: SVD > Baseline > NMF > k-NN > Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hidden Markov model\n",
    "Consider the HMM package https://hmmlearn.readthedocs.io/en/latest/\n",
    "\n",
    "a. Generate sequences with multinomial HMM (2 symbols and 4 hidden states) and given parameters. Start probability - {0.4,0.2,0.1,0.3}, Transition matrix - {{0.2,0.3,0.1,0.4},{0.3,0.3,0.2,0.2},{0.4,0.2,0.3,0.1},{0.2,0.3,0.1,0.4}}, Emission probability - {{0.2,0.8},{0.1,0.9},{0.5,0.5},{0.6,0.4}}.\n",
    "\n",
    "\n",
    "b. Consider a sequence - {1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1}. Fit a multinomial HMM considering 4 states and obtain hidden state which is most likely to have generated the symbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "#a)\n",
    "model = hmm.MultinomialHMM(n_components=4)\n",
    "model.n_features = 2\n",
    "model.startprob_ = np.array([0.4,0.2,0.1,0.3])\n",
    "model.transmat_ = np.array([[0.2,0.3,0.1,0.4],\n",
    "                           [0.3,0.3,0.2,0.2],\n",
    "                           [0.4,0.2,0.3,0.1],\n",
    "                           [0.2,0.3,0.1,0.4]])\n",
    "model.emissionprob_ = np.array([[0.2,0.8],\n",
    "                               [0.1,0.9],\n",
    "                               [0.5,0.5],\n",
    "                               [0.6,0.4]])\n",
    "\n",
    "X,Y = model.sample(10)\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "#b)\n",
    "seq = [[1],[0],[0],[0],[1],[1],[1],[1],[0],[1],[0],[1],[0],[1],[0],[1],[1],[1],[0],[0],[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[1]]\n",
    "remodel = hmm.MultinomialHMM(n_components=4)\n",
    "remodel.fit(seq, [len(seq)])\n",
    "Z = remodel.predict(seq, [len(seq)])\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PrefixSpan \n",
    "Implement the Prefix algorithm for Sequential Pattern Mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in: array of strings, int\n",
    "#out: array of frequent sequences\n",
    "def prefixSpan(database, min_sup):\n",
    "    symbols = dict()\n",
    "    frequents = []\n",
    "    projected = dict()\n",
    "\n",
    "    #mine frequent singles\n",
    "    for seq in database:\n",
    "        for sym in seq:\n",
    "            if sym not in symbols:\n",
    "                symbols[sym] = 1\n",
    "            else:\n",
    "                symbols[sym] += 1\n",
    "\n",
    "    keys = list(symbols.keys())\n",
    "    for key in keys:\n",
    "        if symbols[key] >= min_sup:\n",
    "            frequents.append(key)\n",
    "    singleFrequents = frequents.copy()\n",
    "\n",
    "    for sym in frequents:\n",
    "        projected[sym] = []\n",
    "\n",
    "    for seq in database:\n",
    "        for sym in frequents:\n",
    "            find = seq.find(sym)\n",
    "            if find != -1 and len(seq) > find+1:\n",
    "                projected[sym].append(seq[find+1:])\n",
    "\n",
    "    #print(symbols)\n",
    "    #print(frequents)\n",
    "    #print(projected)\n",
    "\n",
    "    #build every possible seq\n",
    "    foundSeq = frequents.copy()\n",
    "    while len(foundSeq) > 0:\n",
    "        #print(\"Currently found frequents: \")\n",
    "        #print(frequents)\n",
    "        nextSeq = buildSequences(foundSeq, singleFrequents)\n",
    "        #print(\"Now checking:\")\n",
    "        #print(nextSeq)\n",
    "        foundSeq = []\n",
    "        projected = buildProjectedDB(projected, frequents, singleFrequents)\n",
    "        #print(\"Using prjected db:\")\n",
    "        #print(projected)\n",
    "        for seq in nextSeq:\n",
    "            if len(projected[seq]) >= min_sup:\n",
    "                foundSeq.append(seq)\n",
    "        frequents += foundSeq\n",
    "\n",
    "    return frequents\n",
    "\n",
    "#in: list,list\n",
    "#out: list\n",
    "def buildSequences(sequences, symbols):\n",
    "    ret = []\n",
    "    for seq in sequences:\n",
    "        for sym in symbols:\n",
    "            ret.append(seq+sym)\n",
    "    return ret\n",
    "\n",
    "#in: dict, list, list\n",
    "#out: dict\n",
    "def buildProjectedDB(database, latestSequences, singleFrequents):\n",
    "    newDB = database.copy()\n",
    "    #for all frequent sequences found until now\n",
    "    for frSeq in latestSequences:\n",
    "        #for each possible next frequent single appended to a frequent sequence\n",
    "        for sym in singleFrequents:\n",
    "            newDB[frSeq+sym] = []\n",
    "            for prSeq in database[frSeq]:\n",
    "                #this allows sequences with symbols inbetween; amusing\n",
    "                #find = prSeq.find(sym)\n",
    "                #if find != -1 and len(prSeq) > find+1:\n",
    "                #    newDB[frSeq+sym].append(prSeq[find+1:])\n",
    "                if prSeq[0] == sym:\n",
    "                    newDB[frSeq+sym].append(prSeq[1:])\n",
    "    return newDB\n",
    "\n",
    "#test\n",
    "db = [\"612735769345734\",\"347153456\",\"7932465\",\"17253476845\",\"623452386\"]\n",
    "print(\"Frequent (>=2) sequences found:\")\n",
    "print(prefixSpan(db, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
