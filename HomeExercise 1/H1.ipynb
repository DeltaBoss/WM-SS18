{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Web-Mining Home Assignment 1\n",
    "# Deniz Schmidt 334744\n",
    "# Dinc Erduran "
=======
    "# Dinc Erduran 262999"
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 25,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '_' for i in text])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 26,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a\n",
    "# if you use parse instead of query you get a nicer format returned\n",
    "url = \"https://en.wikipedia.org/w/api.php?action=parse&prop=links&page=web_design&format=json\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
=======
   "execution_count": 27,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": null,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done grabbing content.\n"
     ]
    }
   ],
   "source": [
    "for link in r.json()[\"parse\"][\"links\"]:\n",
    "    if link[\"ns\"] == 0 and \"exists\" in link:\n",
    "        file = open(link[\"*\"]+\".txt\",\"w\") \n",
    "        tmp = requests.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=1&explaintext=1&titles=\"+link[\"*\"].replace(\" \",\"_\"))\n",
    "        file.write(remove_non_ascii(list(tmp.json()[\"query\"][\"pages\"].values())[0][\"extract\"]))\n",
    "        file.close() \n",
    "print(\"Done grabbing content.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": null,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "nltk.download('stopwords')\n",
    "stops = set(nltk.corpus.stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": null,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering stopwords.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        orig = open(filename, \"r\")\n",
    "        nostops = open(filename[0:-4]+\".nstxt\", \"w\")\n",
    "        \n",
    "        filtered_words = [word for word in orig.read().lower().replace(\",\",\"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\" \") if word not in stops]\n",
    "        \n",
    "        nostops.write(\" \".join(filtered_words))\n",
    "        \n",
    "        orig.close()\n",
    "        nostops.close()\n",
    "        \n",
    "print(\"Done filtering stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": null,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "#both have trouble with singular-plural\n",
    "#stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": null,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done stemming.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".nstxt\"):\n",
    "        orig = open(filename, \"r\")\n",
    "        stemmed = open(filename[0:-6]+\".stxt\", \"w\")\n",
    "        \n",
    "        words = orig.read().split(\" \")\n",
    "        swords = []\n",
    "        for word in words:\n",
    "            swords.append(stemmer.stem(word))\n",
    "            \n",
    "        \n",
    "        stemmed.write(\" \".join(swords))\n",
    "        \n",
    "        orig.close()\n",
    "        stemmed.close()\n",
    "print(\"Done stemming.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Done creating term-document matrix.\n"
=======
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#2.3\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "documents = []\n",
    "titles = []\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        titles.append(os.path.basename(filename))\n",
    "        tmp = open(filename,\"r\")\n",
    "        documents.append(tmp.read())\n",
    "        tmp.close()\n",
    "documents.insert(0, \"web development design\")\n",
    "titles.insert(0, \"query\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "tdm2 = np.array(term_vectors.toarray())\n",
    "# 2.3) if it has to be term as i doc as j\n",
    "print(tdm2.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf cosine similarity top 5\n",
      "query\n",
      "Adaptive web design.txt\n",
      "Design education.txt\n",
      "Web development.txt\n",
      "Tableless web design.txt\n",
      "Web developer.txt\n",
      "tf idf cosine similarity top 5\n",
      "query\n",
      "Web development.txt\n",
      "Web developer.txt\n",
      "Adaptive web design.txt\n",
      "Web engineering.txt\n",
      "Web 2.0.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "# for 3.1a)\n",
    "tf = transformer.fit_transform(term_vectors.toarray())\n",
    "\n",
    "\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "# for 3.1b)\n",
    "tfidf = transformer.fit_transform(term_vectors.toarray())\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# added the query \"web development design\" as the first row \n",
    "cosine_similarities_tf = linear_kernel(tf[0:1], tf).flatten()\n",
    "top5_tf = cosine_similarities_tf.argsort()[:-7:-1]\n",
    "print(\"tf cosine similarity top 5\")\n",
    "for i in range(0, 6):\n",
    "  print (titles[top5_tf[i]])\n",
    "\n",
    "\n",
    "cosine_similarities_tfidf = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "top5_tfidf = cosine_similarities_tfidf.argsort()[:-7:-1]\n",
    "print(\"tf idf cosine similarity top 5\")\n",
    "for i in range(0, 6):\n",
    "  print (titles[top5_tfidf[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "Adaptive web design.txt\n",
      "Continuous design.txt\n",
      "Web developer.txt\n",
      "Design education.txt\n",
      "Empathic design.txt\n"
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
     ]
    }
   ],
   "source": [
    "#3.2\n",
    "import itertools\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocdict = dict(itertools.zip_longest(vocab, range(len(vocab))))\n",
    "\n",
<<<<<<< HEAD
    "for document in documents:\n",
    "    col = len(terms)*[0]\n",
    "    for word in document:\n",
    "        if word in terms:\n",
    "            col[terms.index(word)] += 1\n",
    "        else:\n",
    "            terms.append(word)\n",
    "            col.append(1)\n",
    "    tdm.append(col)\n",
    "    \n",
    "for col in tdm:\n",
    "    while len(col) < len(terms):\n",
    "        col.append(0)\n",
    "    \n",
    "print(\"Done creating term-document matrix.\")"
=======
    "i = 0\n",
    "probs = {}\n",
    "for row in term_vectors.toarray():\n",
    "  total = (1.0/max(1, np.sum(row)))\n",
    "  p = row[vocdict['web']]*total + row[vocdict['development']]*total + row[vocdict['design']]*total\n",
    "  probs[i] = p\n",
    "  i += 1\n",
    "\n",
    "import operator\n",
    "sorted_probs = sorted(probs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for i in range(0, 6):\n",
    "  print (titles[sorted_probs[i][0]])"
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "\n",
    "txts = []\n",
    "titles = []\n",
    "for file in glob.glob(\"*.stxt\"):\n",
    "  titles.append(os.path.basename(file))\n",
    "  with open(file, \"r\") as doc:\n",
    "    txts.append(doc.read())\n",
    "\n",
    "txts.insert(0, \"web development design\")\n",
    "titles.insert(0, \"query\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_vectors = vectorizer.fit_transform(txts)\n",
    "\n",
    "\n",
    "tdm = np.array(term_vectors.toarray())\n",
    "# 2.3) if it has to be term as i doc as j\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "# for 3.1a)\n",
    "tf = transformer.fit_transform(term_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "Adaptive web design.stxt\n",
      "Design education.stxt\n",
      "Property designer.stxt\n",
      "Participatory design.stxt\n",
      "Design leadership.stxt\n",
      "query\n",
      "Web development.stxt\n",
      "Adaptive web design.stxt\n",
      "Web developer.stxt\n",
      "Web engineering.stxt\n",
      "Web 2.0.stxt\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "# for 3.1b)\n",
    "tfidf = transformer.fit_transform(term_vectors.toarray())\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# added the query \"web development design\" as the first row \n",
    "cosine_similarities_tf = linear_kernel(tf[0:1], tf).flatten()\n",
    "top5_tf = cosine_similarities_tf.argsort()[:-7:-1]\n",
    "for i in range(0, 6):\n",
    "    print(titles[top5_tf[i]])\n",
    "\n",
    "\n",
    "cosine_similarities_tfidf = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "top5_tfidf = cosine_similarities_tfidf.argsort()[:-7:-1]\n",
    "for i in range(0, 6):\n",
    "    print(titles[top5_tfidf[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'itertools' has no attribute 'izip_longest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-43c44aa02fe4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#3.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvocdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mizip_longest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'itertools' has no attribute 'izip_longest'"
     ]
    }
   ],
   "source": [
    "#3.2\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocdict = dict(itertools.izip_longest(vocab, range(len(vocab))))\n",
    "\n",
    "i = 0\n",
    "probs = {}\n",
    "for row in term_vectors.toarray():\n",
    "    total = (1.0/max(1, np.sum(row)))\n",
    "    p = row[vocdict['web']]*total + row[vocdict['development']]*total + row[vocdict['design']]*total\n",
    "    probs[i] = p\n",
    "    i += 1\n",
    "\n",
    "import operator\n",
    "sorted_probs = sorted(probs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for i in range(0, 6):\n",
    "    print(titles[sorted_probs[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
>>>>>>> 2d4d06f875a469df245be0ba47f0670b104e1443
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
