{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinc Erduran 262999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '_' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a\n",
    "# if you use parse instead of query you get a nicer format returned\n",
    "url = \"https://en.wikipedia.org/w/api.php?action=parse&prop=links&page=web_design&format=json\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in r.json()[\"parse\"][\"links\"]:\n",
    "    if link[\"ns\"] == 0 and \"exists\" in link:\n",
    "        file = open(link[\"*\"]+\".txt\",\"w\") \n",
    "        tmp = requests.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=1&explaintext=1&titles=\"+link[\"*\"].replace(\" \",\"_\"))\n",
    "        file.write(remove_non_ascii(list(tmp.json()[\"query\"][\"pages\"].values())[0][\"extract\"]))\n",
    "        file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "nltk.download('stopwords')\n",
    "stops = set(nltk.corpus.stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        orig = open(filename, \"r\")\n",
    "        nostops = open(filename[0:-4]+\".nstxt\", \"w\")\n",
    "        \n",
    "        filtered_words = [word for word in orig.read().lower().replace(\",\",\"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\" \") if word not in stops]\n",
    "        \n",
    "        nostops.write(\" \".join(filtered_words))\n",
    "        \n",
    "        orig.close()\n",
    "        nostops.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "#both have trouble with singular-plural\n",
    "#stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".nstxt\"):\n",
    "        orig = open(filename, \"r\")\n",
    "        stemmed = open(filename[0:-6]+\".stxt\", \"w\")\n",
    "        \n",
    "        words = orig.read().split(\" \")\n",
    "        swords = []\n",
    "        for word in words:\n",
    "            swords.append(stemmer.stem(word))\n",
    "            \n",
    "        \n",
    "        stemmed.write(\" \".join(swords))\n",
    "        \n",
    "        orig.close()\n",
    "        stemmed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#2.3\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "documents = []\n",
    "titles = []\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        titles.append(os.path.basename(filename))\n",
    "        tmp = open(filename,\"r\")\n",
    "        documents.append(tmp.read())\n",
    "        tmp.close()\n",
    "documents.insert(0, \"web development design\")\n",
    "titles.insert(0, \"query\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "tdm2 = np.array(term_vectors.toarray())\n",
    "# 2.3) if it has to be term as i doc as j\n",
    "print(tdm2.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf cosine similarity top 5\n",
      "query\n",
      "Adaptive web design.txt\n",
      "Design education.txt\n",
      "Web development.txt\n",
      "Tableless web design.txt\n",
      "Web developer.txt\n",
      "tf idf cosine similarity top 5\n",
      "query\n",
      "Web development.txt\n",
      "Web developer.txt\n",
      "Adaptive web design.txt\n",
      "Web engineering.txt\n",
      "Web 2.0.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "# for 3.1a)\n",
    "tf = transformer.fit_transform(term_vectors.toarray())\n",
    "\n",
    "\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "# for 3.1b)\n",
    "tfidf = transformer.fit_transform(term_vectors.toarray())\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# added the query \"web development design\" as the first row \n",
    "cosine_similarities_tf = linear_kernel(tf[0:1], tf).flatten()\n",
    "top5_tf = cosine_similarities_tf.argsort()[:-7:-1]\n",
    "print(\"tf cosine similarity top 5\")\n",
    "for i in range(0, 6):\n",
    "  print (titles[top5_tf[i]])\n",
    "\n",
    "\n",
    "cosine_similarities_tfidf = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "top5_tfidf = cosine_similarities_tfidf.argsort()[:-7:-1]\n",
    "print(\"tf idf cosine similarity top 5\")\n",
    "for i in range(0, 6):\n",
    "  print (titles[top5_tfidf[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "Adaptive web design.txt\n",
      "Continuous design.txt\n",
      "Web developer.txt\n",
      "Design education.txt\n",
      "Empathic design.txt\n"
     ]
    }
   ],
   "source": [
    "#3.2\n",
    "import itertools\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocdict = dict(itertools.zip_longest(vocab, range(len(vocab))))\n",
    "\n",
    "i = 0\n",
    "probs = {}\n",
    "for row in term_vectors.toarray():\n",
    "  total = (1.0/max(1, np.sum(row)))\n",
    "  p = row[vocdict['web']]*total + row[vocdict['development']]*total + row[vocdict['design']]*total\n",
    "  probs[i] = p\n",
    "  i += 1\n",
    "\n",
    "import operator\n",
    "sorted_probs = sorted(probs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for i in range(0, 6):\n",
    "  print (titles[sorted_probs[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
